# 02 - Hacking a Chat Bot

## 02.01 - Manual Testing

Honestly, this is my favorite part of the gig.

Using your RI (real intelligence) against an artificial one (AI). To me, I imagine that this is a venn diagram, where my target is the overlap between something with all of the technical vulnerabilities I am accustomed to, and it's **also** subject to social engineering and weird encoding tricks.

This is the best place to start talking about what you should be looking for.

### MITRE ATLAS

[MITRE ATLAS](https://atlas.mitre.org/matrices/ATLAS) is like MITRE ATT&CK, but for AIs! This is a taxonomy of all of the bad things that an attacker may try to do against an AI or LLM.

Here are some common attacks that we're talking about today represented within ATLAS:
* [System Prompt Discovery](https://atlas.mitre.org/techniques/AML.T0069.002)
* [Prompt Injection](https://atlas.mitre.org/techniques/AML.T0051)
* [Jailbreaking](https://atlas.mitre.org/techniques/AML.T0054)

### Arcanum Prompt Injection Taxonomy

The folks at Arcanum have done a REALLY good job creating a taxonomy for how prompt injection works in detail. You can reference that [here](https://arcanum-sec.github.io/arc_pi_taxonomy/).

What we like about this is that it breaks out intents, which are what you're trying to achieve, and provides really robust descriptions of different evasions to apply when you're sending the prompt.

### Exercise

Look over these resources and attempt to apply them to the chatbot you wrote previously.

Look over the [Arcanum Prompt Injection Taxonomy](https://arcanum-sec.github.io/arc_pi_taxonomy/) and apply some intents and evasions to things you have tried so far.

If you find combos that work for you, write them down! Maintain a library of good prompts.
