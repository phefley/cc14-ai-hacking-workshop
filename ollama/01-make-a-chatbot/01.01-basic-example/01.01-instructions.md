# 1 - Building a Chatbot

## 1.1 - Simple example

This is a simple example to make sure that your environment is working correctly.

When you look at the code, it has three distinct sections:

### Setup

This is setting up global variables for what model to use. By default, all of the examples here are set up to use the SMALL model we proposed in the requirements. You can edit that here and use the larger one if you'd like for more robust responses.
This section is common through all of the ollama examples we supply. We'll move beyond just the ollama library the further down the examples we go, as we'll need to add more functionality.

```python
import ollama

BIG_MODEL_NAME = 'llama3:8b'
SMALL_MODEL_NAME = 'llama3.2:1b'

MODEL_NAME = SMALL_MODEL_NAME
```

### A simple request

This is a simple chat completion using the python ollama library. You send a message and get a response. This prints it out!
You can change the content of the message to test interacting with the ollama service.

```python
# Simple chat completion
response = ollama.chat(model=MODEL_NAME, messages=[
  {
    'role': 'user',
    'content': 'Why did the cat sit on the computer?',
  },
])
print(response['message']['content'])
```

### A streamed request

Sometimes, you want to get the response as it comes (in chunks) from the LLM and not wait. This is a good example of how to run a streaming request and get the response in chunks, which we print out as we go. Writing a story takes a while, so this is a good example of not having to wait until it's done!


```python
# Streaming responses
stream = ollama.chat(
    model=MODEL_NAME,
    messages=[{'role': 'user', 'content': 'Tell me a story'}],
    stream=True,
)

for chunk in stream:
  print(chunk['message']['content'], end='', flush=True)
```

### Running the code

Once you're in the ollama virtual environment we setup, you can run this with:

```bash
python 01.01-basic-example.py
```

