# 1 - Building a Chatbot

## 1.8 - Model Context Protocol (MCP)

The Model Context Protocol (MCP) was open-sourced by Anthropic in November 2024. MCP is an open standard that enables AI systems to connect with data sources, tools, and environments. It's often referred as the "USB-C for AI applications" to evoke the "plug-and-play" nature of the concept.

### Core Components

**MCP Host:** The AI application (e.g., Claude, ChatGPT) that manages connections to external systems.
**MCP Client:** A connector that links the host to a specific data source.
**MCP Server:** A program that exposes data or tools to the AI via the protocol.

An MCP client can be configured to interact with multiple servers, to expand what the model deployment is capable of.

### Context Primitives

The building blocks for interaction:

- **Tools:** Executable functions (e.g., API calls, database queries).
- **Resources:** Data sources (e.g., files, records, API responses).
- **Prompts:** Reusable templates for getting expected, repeatable results.

Clients can discover and invoke primitives dynamically.

### Transport Layer

- **STDIO Transport:** For local connections (what we use here).
- **Streamable HTTP Transport:** For remote connections, supporting streaming and secure authentication (OAuth, API keys).

---

## What Changed from 01.07 to 01.08

This section walks through every major change between the RAG-only chatbot (01.07) and the MCP-enabled version (01.08).

### New File: `01.08-mcp-server.py`

This is a standalone MCP server that demonstrates all three context primitives. The host launches it automatically as a subprocess via STDIO — you do **not** run it separately.

```python
from mcp.server.fastmcp import FastMCP

mcp = FastMCP("demo-server")

@mcp.tool()
def get_system_time() -> str:
    """Return the current system date and time."""
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

@mcp.resource("info://about")
def about() -> str:
    """Describes what this MCP server does."""
    return "This is a demo MCP server that exposes three primitives: ..."

@mcp.prompt()
def summarize_for_executive(topic: str) -> str:
    """Create an executive summary prompt for a given topic."""
    return f"Summarize the following topic for a C-level executive. ... Topic: {topic}"
```

### New File: `mcp_config.json`

Tells the host which MCP servers to launch and connect to. The host reads this at startup and spawns each server as a child process.

```json
{
    "mcpServers": {
        "local-tools": {
            "command": "python",
            "args": ["01.08-mcp-server.py"]
        }
    }
}
```

### New Imports & Setup

01.07 had no MCP dependencies. 01.08 adds the MCP client SDK, async utilities, and `nest_asyncio` (needed because LangGraph's sync `invoke()` runs inside an async FastAPI context):

```python
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from contextlib import AsyncExitStack
import json, os, asyncio, nest_asyncio

nest_asyncio.apply()
```

### New: `MCPClientManager` Class

This is the entire MCP client implementation (~250 lines). It:

1. **Reads `mcp_config.json`** to discover configured servers.
2. **Launches each server** as a subprocess via STDIO transport.
3. **Discovers all three primitive types** — calls `session.list_tools()`, `session.list_resources()`, and `session.list_prompts()` on each server.
4. **Stores metadata** in `self.tools`, `self.resources`, and `self.prompts` dictionaries, keyed by `server_name.primitive_name`.
5. **Provides methods to use them:** `call_tool()`, `read_resource()`, `get_prompt()`.
6. **Provides description methods** for each primitive type, used to build LLM decision prompts.

### Changed: `AgentState` — New `tool_output` Field

01.07's state only tracked messages and RAG context:

```python
# 01.07
class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], operator.add]
    context: str
```

01.08 adds a `tool_output` field to carry MCP results through the graph:

```python
# 01.08
class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], operator.add]
    context: str
    tool_output: str
```

### New: `check_and_call_tools` Node

This is a new LangGraph node that asks the LLM whether any MCP primitive should be used. It builds a decision prompt listing all available tools, resources, and prompts, then parses the LLM's structured response (`CALL_TOOL:`, `READ_RESOURCE:`, `GET_PROMPT:`, or `NO_TOOL`) and executes the appropriate MCP call.

### Changed: `generate_response` — Two Prompt Templates

01.07 always sent the RAG context to the LLM. 01.08 uses two different prompts depending on whether a tool was called:

- **Tool was called:** A short, focused prompt with just the tool result. The RAG context is omitted entirely to prevent the model from getting confused by irrelevant documents.
- **No tool called:** The original RAG prompt with knowledge base context and conversation history.

### Changed: Graph Flow — Conditional Routing

01.07 had a simple linear graph:

```
start --> retrieve --> generate --> end
```

01.08 checks tools first and conditionally skips RAG retrieval when a tool produces output:

```
start --> check_tools --[tool fired]--> generate --> end
                      \--[no tool]----> retrieve --> generate --> end
```

This avoids wasting time on a vector similarity search when the answer already came from a tool.

### Changed: FastAPI Lifespan (replaces `on_event`)

01.07 used the now-deprecated `@api.on_event("startup")` pattern. 01.08 uses the modern `lifespan` context manager, which also initializes MCP connections:

```python
@asynccontextmanager
async def lifespan(app):
    global vectorstore, embeddings, llm
    embeddings = OllamaEmbeddings(model="nomic-embed-text")
    vectorstore = Chroma(persist_directory="./chroma_db", embedding_function=embeddings)
    llm = OllamaLLM(model=MODEL_NAME)
    await mcp_manager.connect_all("mcp_config.json")    # NEW
    yield
    await mcp_manager.cleanup()                          # NEW

api = FastAPI(lifespan=lifespan)
```

### Changed: `ChatResponse` — New `tool_used` Field

The API response now includes what tool/resource/prompt was used (if any):

```python
# 01.07
class ChatResponse(BaseModel):
    response: str
    context_used: str

# 01.08
class ChatResponse(BaseModel):
    response: str
    context_used: str
    tool_used: str = ""
```

### New: MCP REST Endpoints

Six new FastAPI endpoints for direct MCP interaction (bypassing the chatbot/LLM):

| Method | Path | Purpose |
|--------|------|---------|
| `GET` | `/mcp/tools` | List all discovered tools |
| `POST` | `/mcp/call-tool` | Call a tool by qualified name |
| `GET` | `/mcp/resources` | List all discovered resources |
| `POST` | `/mcp/read-resource` | Read a resource by qualified name |
| `GET` | `/mcp/prompts` | List all discovered prompts |
| `POST` | `/mcp/get-prompt` | Render a prompt with arguments |
| `POST` | `/mcp/reconnect` | Reconnect to all MCP servers |

### Changed: Timeout Configuration

The 8b model can take over a minute per LLM call, and there are two calls per user message (decision + response). The uvicorn keep-alive timeout is set to 300 seconds, and the browser fetch uses a 5-minute `AbortController` timeout.

---

## Running It

Once you're in the ollama virtual environment we setup, you can run this with:

```bash
python 01.08-mcp-host.py
```

You do **not** need to run the MCP server separately — the host launches it automatically based on `mcp_config.json`.

Access the UI at `http://127.0.0.1:8000/static/mcpchat.html`.

## Testing It

### cURL testing

Chat endpoint (same as 01.07):
```bash
curl -X POST "http://localhost:8000/chatbot/" \
  -H "Content-Type: application/json" \
  -d '{"message": "What time is it?", "session_id": "test-session-1"}'
```

List discovered MCP tools:
```bash
curl http://localhost:8000/mcp/tools
```

List discovered MCP resources:
```bash
curl http://localhost:8000/mcp/resources
```

Read a resource directly:
```bash
curl -X POST "http://localhost:8000/mcp/read-resource" \
  -H "Content-Type: application/json" \
  -d '{"resource_key": "local-tools.about"}'
```

Call a tool directly:
```bash
curl -X POST "http://localhost:8000/mcp/call-tool" \
  -H "Content-Type: application/json" \
  -d '{"tool_key": "local-tools.get_system_time", "arguments": {}}'
```

List discovered MCP prompts:
```bash
curl http://localhost:8000/mcp/prompts
```

Render a prompt:
```bash
curl -X POST "http://localhost:8000/mcp/get-prompt" \
  -H "Content-Type: application/json" \
  -d '{"prompt_key": "local-tools.summarize_for_executive", "arguments": {"topic": "cloud migration"}}'
```

## Model Considerations

The MCP plumbing works with any Ollama model, but the LLM's ability to correctly decide when to call tools and use the results varies significantly by model size:

| Model | Calls tools correctly? | Uses result in answer? | Speed |
|-------|----------------------|----------------------|-------|
| `llama3:8b` | Yes | Yes | Slow (~1-2 min) |
| `llama3.2:3b` | Yes | No | Medium |
| `llama3.2:1b` | Yes (but too eagerly) | No | Fast |

This illustrates the tradeoff and capabilities of using bigger or smaller models.
Smaller models run more quickly but lack the precision to be effective at tasks like this.

## Exercises

* Try asking "What time is it?" and watch the terminal output to see the tool being called.
* Try the direct MCP endpoints (`/mcp/tools`, `/mcp/read-resource`, etc.) with cURL to see primitives without the LLM in the loop.
* Add a new tool to the MCP server (e.g., a calculator, a random number generator) and restart the host — it will be discovered automatically
* Can you write a second MCP server and add it to `mcp_config.json`? The host supports multiple servers.
