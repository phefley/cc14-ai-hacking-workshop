# 1 - Building a Chatbot

## 1.7 - RAG Chat

Retrieval augmented generation (RAG) can be used to provide more up to date, specific, or private information. Think of it this way - you're giving more information to an already trained model without having to do more training.

You can use local files, maybe things unique to your organization or things you want to keep private, or even empower the LLM to search the web to obtain content you're looking for, enhacing what it already knows.

To do this, we're going to expand beyond langchain in to [langgraph](https://docs.langchain.com/oss/python/langgraph/agentic-rag).


LangGraph allows you to build a graph, something with nodes and edges, along which the system will follow the graph. Think of it kind of like a wireframe or process diagram where you have each step connected with lines.

[This](https://medium.com/the-ai-forum/build-a-reliable-rag-agent-using-langgraph-2694d55995cd) is a really good writeup on how one would achieve this with LangGraph.

### Code Review

Let's step through the code blocks that have changed significantly since our last example.

The first thing of note is a function to initialize the vector store. This is where we're putting all of the text data that we have.

```python
VECTORSTORE_DB_PATH = "./chroma_db"

#...

# ============ RAG Setup (one-time) ============
def setup_vectorstore():
    """Run this once to create your vector database"""
    print("[+] Setting up vector store...")
    loader = DirectoryLoader('./docs', glob="**/*.txt", loader_cls=TextLoader)
    documents = loader.load()
    for doc in documents:
        print(f"\t* Loaded document: {doc.metadata['source']} (length: {len(doc.page_content)})")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    chunks = text_splitter.split_documents(documents)
    
    embeddings = OllamaEmbeddings(model="nomic-embed-text")
    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=VECTORSTORE_DB_PATH
    )
    print(f"[+] Persisting vector store to {VECTORSTORE_DB_PATH} ...")
    return vectorstore
```

Note that we're loading all of the `.txt` files in `./docs`. Take some time to check [that](./docs/) out now.

The next section is creating our `AgentState` class for tracking context and the conversation as we go. We then go through the process of initializing the text embeddings model, the Chroma vectorstore, and the Ollama interface.

```python
# ============ LangGraph RAG with Memory ============

class AgentState(TypedDict):
    """State schema for our RAG agent"""
    messages: Annotated[Sequence[BaseMessage], operator.add]
    context: str

# Initialize components (do this at module level or in FastAPI startup)
embeddings = OllamaEmbeddings(model="nomic-embed-text")
vectorstore = Chroma(
    persist_directory=VECTORSTORE_DB_PATH,
    embedding_function=embeddings
)
llm = OllamaLLM(model=MODEL_NAME)

```

Next, we define a few functions we'll need:
* Retrieve Context - the RAG bit
* Generate Response - get the LLM's response

The retrieve context function uses `vectorstore.similarity_search` to find things in our vector database, which we loaded from those text files, which are similar to the last user message.

```python
def retrieve_context(state: AgentState):
    """Retrieve relevant documents based on the latest user message"""
    # Get the last user message
    last_message = state["messages"][-1].content
    
    # Search vector store
    docs = vectorstore.similarity_search(last_message, k=3)
    
    # Combine documents into context
    context = "\n\n".join([
        f"Document {i+1}:\n{doc.page_content}" 
        for i, doc in enumerate(docs)
    ])
    
    return {"context": context}
```

The generate response function is where we take all of the previous message history, the relevant context we got from that retrieval function, and send it all along to the LLM in the prompt to get a response. This is where the system prompt is defined.

```python
def generate_response(state: AgentState):
    """Generate response using LLM with retrieved context and conversation history"""
    context = state.get("context", "")
    messages = state["messages"]
    
    # Build conversation history
    conversation = "\n".join([
        f"{'User' if isinstance(msg, HumanMessage) else 'Assistant'}: {msg.content}"
        for msg in messages[:-1]  # All except the last message
    ])
    
    # Get the current question
    current_question = messages[-1].content
    
    # Create prompt with context and history
    prompt = f"""You are a helpful assistant. Use the following context to answer the user's question. If the answer cannot be found in the context, say so.

Context from knowledge base:
{context}

Previous conversation:
{conversation}

Current question: {current_question}

Answer:"""
    
    # Generate response
    response = llm.invoke(prompt)
    
    # Return as AIMessage
    return {"messages": [AIMessage(content=response)]}
```

Finally, we build the graph. This essentially builds out a two node graph (four if you count the start and end):

```
start --> retrieve --> generate --> end
```

We also make sure we add a memory checkpointer to keep track of the conversation.

```python
# Build the graph
workflow = StateGraph(AgentState)

# Add nodes
workflow.add_node("retrieve", retrieve_context)
workflow.add_node("generate", generate_response)

# Define flow
workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "generate")
workflow.add_edge("generate", END)

# Compile with memory
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)
```

The rest should look the same!

### Running It

Once you're in the ollama virtual environment we setup, you can run this with:

```bash
python 01.07-langgraph-rag.py
```

We have a prettier UI now. You can access that at `http://127.0.0.1:8000/static/ragchat.html`.


### Testing It

For the testing of 01-07, please note that we moved the parameters around a little and renamed them. You need to pass a `message` and `session_id` in your JSON.

#### cURL testing
Use the following sequence to maintain a conversation:

```bash
curl -X POST "http://localhost:8000/chatbot/" \
  -H "Content-Type: application/json" \
  -d '{"message": "Do you remember my name?", "session_id": "8b8834c6-8ee7-49dd-a7f4-cc1de64877eb"}'
```

#### Python Testing
There is also an example of how to do this with the Python requests library supplied.

*Note:* This example takes a second, optional command line parameter: the thread identifier. It also returns the string rendition of the JSON returned by the API, which includes the threadId value. Just in case you want to ask questions and maintain the thread.

```bash
python 01.07-tester.py "Hi my name is Bob. Can you tell me about the history of Python?"
python 01.07-tester.py "Hi! Do you remember my name? Can you tell me about lists?" 4becb36c-f518-4165-8746-4ec76a371b1a
```


### Exercises

* Can you get the flag to show up in the answer from the API? **NOTE**: The answer is a field of the JSON. The whole context is being shared and that includes the flag for academic purposes. If we removed that... would you stil be able to get the flag?
* Given that you can modify the system prompt and the data supplied in the text files, what other cool things can you do with this?
* It's a little late now, but our friends at [HackerTracker](https://hackertracker.app) has a [schedule](https://hackertracker.app/schedule?conf=CACTUSCON2026) posted online for this. Can you use the text of the schedule to build a helpful schedule bot?
* There are [ways](https://docs.langchain.com/oss/python/integrations/document_loaders/async_chromium) to pull in the data using a chromium browser. Can you either pull in the conference schedule using that or another page full of information?
