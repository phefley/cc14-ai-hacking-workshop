# 1 - Building a Chatbot

## 1.6 - Giving the chatbot some memory!

Now that we have langchain in place, we're going to use it to give the chatbot some memory. You can read more about how they do this at the [langchain docs](https://docs.langchain.com/oss/python/langchain/short-term-memory).


### Code Review

We're using the [ChatOllama](https://docs.langchain.com/oss/python/integrations/chat/ollama) object to integrate with our langchain agent.

```python
model = ChatOllama(
    model=MODEL_NAME,
    validate_model_on_init=True,
    #temperature=0.8,
    #num_predict=256,
    # other params ...
)

tools=[]
agent = create_agent(model, tools=tools, checkpointer=InMemorySaver())

system_prompt = SystemMessage("""
You are a senior Python developer.
Always provide code examples and explain your reasoning.
Be concise but thorough in your explanations.
When providing code examples, ensure they are properly formatted and syntactically correct.
You can only discuss the python programming language. Do not discuss plants or succulents. Refuse to discuss anything other than python.
""")
```

We're also creating a system prompt, which we'll use as the first message in any new conversation. Here, the example is to help you write python code, but you can update this to be anything you want!!

Next, we do a lot with FastAPI. Really, what we're doing here is adding a CORS configuration so we can call the API from our pretty new UI, which we're then staging as a static file:

```python
app = FastAPI()

origins = [
    "http://localhost",
    "http://localhost:8000",
    "http://127.0.0.1",
    "http://127.0.0.1:8000",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.mount("/static", StaticFiles(directory="static"), name="static")
```

We modify our "ask_ollama" function to handle the addition of threads, or conversations. The new function takes our usual `question`, but it also takes a `threadId` which will help us identify which thread this all belongs to. It also has a parameter `firstTimeCaller` to determine if this is the first time someone is starting a conversation.
Why does that matter? Well - if they're a first time caller, we have no history for them and the chat for them hasn't been set up with the system prompt! That's what this function does.
It will return the LAST message in the thread, which will be the agent's response to our last HumanMessage we sent it.

```python
def ask_ollama(question: str, threadId: str, firstTimeCaller: bool = True) -> str:
    if firstTimeCaller:
        messages = [system_prompt, HumanMessage(content=question)]
    else:
        messages = [HumanMessage(content=question)]
    
    response = agent.invoke(
      {"messages": messages},
      {"configurable": {"thread_id": threadId}},  
    )
    for message in response['messages']:
        last_message = message.content
    return last_message
```

Next let's look at out main method for the `/chatbot` endpoint.
Not a lot has changed here, either!! We do check to see if there's a threadId sent along. If not, then that's a first time caller and they need a threadId set. We're using a UUID4 because I'm lazy.

```python
@app.post("/chatbot/")
async def chat(request: Request):
    data = await request.json()
    
    # Access the data directly
    userQuestion = data.get("question")
    threadId = data.get("threadId")
    if not threadId:
        threadId = str(uuid.uuid4())
        firstTimeCaller = True
    else:
        firstTimeCaller = False
    
    question = userQuestion
    
    return {
        "answer": ask_ollama(question, threadId, firstTimeCaller),
        "threadId": threadId,
    }
```


### Running It

Once you're in the ollama virtual environment we setup, you can run this with:

```bash
python 01.06-langchain-memory.py
```

For the first time in our examples, we're also adding a pretty UI. You can access that at `http://127.0.0.1:8000/static/chat.html`.

Looking like a chatbot now!

### Testing It
Testing is the same here as 01.04. Might be worth trying a few things out to see if you get different responses!

#### cURL testing
Use the following sequence to maintain a conversation:

```bash
curl -X POST "http://localhost:8000/chatbot/" \
  -H "Content-Type: application/json" \
  -d '{"question": "Hi my name is Bob. Can you tell me about the history of Python?"}'
```

When you get the response, copy the threadId and use it to ask a follow-up question like this:

```bash
curl -X POST "http://localhost:8000/chatbot/" \
  -H "Content-Type: application/json" \
  -d '{"question": "Do you remember my name?", "threadId": "8b8834c6-8ee7-49dd-a7f4-cc1de64877eb"}'
```

#### Python Testing
There is also an example of how to do this with the Python requests library supplied.

*Note:* This example takes a second, optional command line parameter: the thread identifier. It also returns the string rendition of the JSON returned by the API, which includes the threadId value. Just in case you want to ask questions and maintain the thread.

```bash
python 01.06-tester.py "Hi my name is Bob. Can you tell me about the history of Python?"
python 01.06-tester.py "Hi! Do you remember my name? Can you tell me about lists?" 4becb36c-f518-4165-8746-4ec76a371b1a
```


### Exercise

Modify the system prompt. What else could you purpose this chatbot for?
Look over the documentation for chat permanence. This is all done in memory, but could you use a database to keep chats living a longer time?

Did you notice the following line?

```python
tools = []
```

What more could you do with that? For hints, check out the [langchain documentation](https://docs.langchain.com/oss/python/langchain/tools).

### Challenge

There's a trivial [jailbreak](https://atlas.mitre.org/techniques/AML.T0054) in this example. Can you find it?

If you want a challenge - get it to talk about cacti.
If you want a *real* challenge - get it to not talk about cacti, despite what you know.

### Summary

WOW! This is feeling very chat-bot-ty around here. I think we're in a really good place here with this chatbot. What else could we add? I mean, it would be nice if we could give this thing some context to speak to...
