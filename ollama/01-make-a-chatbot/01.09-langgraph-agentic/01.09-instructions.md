# 1 - Building a Chatbot

## 1.9 - (BONUS) Agentic AI with Tools

Using the capability of LangGraph, we're going to make a silly example. Imagine a library AI chatbot. You'll probably need to be able to look up your library card!

The docs for adding tools to langchain an langgraph are available [here](https://docs.langchain.com/oss/python/langchain/tools#toolnode).

We added a [sqlite database](./librarycards.sqlite) of library cards. This is what the CARDS table looks like in CSV:

```
FirstName,LastName,ID,CardNumber
Peter,Hefley,1,10000001
Susan,Lopez,2,10000002
Thomas,Anderson,3,10000003
Dade,Murphy,4,10000004
Jennifer,Smith,5,10000005
Daniel,Adams,6,10000006
```


### Code Review

Again - our focus is on the changes. Let's take a look!

We import sqlite3, set the `LIBRARY_DB` file name, and set the `MODEL_NAME`. Notably, here we use gpt-oss. We had to do this because tool support is not uniform. This example uses gpt-oss and we're terribly sorry :-)

```python
import sqlite3
#...
LIBRARY_DB = "./librarycards.sqlite"

MODEL_NAME = "gpt-oss" # Note -- this is for good tool support.
```

Now, we define the tool we'll use. The tool `get_library_card_number` takes in a `firstName` and a `lastName` and tries to look up the card using SQL.

```python
# Define your tools
@tool
def get_library_card_number(firstName: str, lastName: str) -> str:
    """Get the library card number for a person given their first and last name.
    
    Args:
        firstName: The first name of the person
        lastName: The last name of the person
    """
    print(f"[+] Tool called: get_library_card_number for {firstName} {lastName}")
    # Connect to your database
    conn = sqlite3.connect(LIBRARY_DB)  # Replace with your database file path
    cursor = conn.cursor()
    cursor.execute("SELECT CardNumber FROM CARDS WHERE FirstName = ? AND LastName = ?", (firstName, lastName))
    cardNum = cursor.fetchone()
    if cardNum:
        return f"The library card number for {firstName} {lastName} is {cardNum[0]}"
    else:
        return f"No library card found for {firstName} {lastName}"
```

Next, we do some housekeeping for having tools.
We create a list of our tools, we bind those tools to the `llm` object and store the returned value. Then, we make a langgraph built-in `ToolNode` with our tools.

```python
tools = [get_library_card_number]

# Bind tools to your LLM
llm_with_tools = llm.bind_tools(tools)

# Create a ToolNode - this automatically executes tool calls
tool_node = ToolNode(tools)
```

Now, we add a conditional check for our state which determines if we're done or if we need to call tools.

```python
def should_continue(state: AgentState) -> Literal["tools", "end"]:
    """Determine if we need to call tools or end"""
    last_message = state["messages"][-1]
    
    # If there are tool calls, route to tools node
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    
    # Otherwise, we're done
    return "end"
```

Now we modify the flow. We still retrieve and generate, but after that - we check to see if we need to run tools or if we're done. If we run tools, there is an additional generation (i.e., call to the LLM) to process the tool output.

```python
# Build the graph
workflow = StateGraph(AgentState)

# Add nodes
workflow.add_node("retrieve", retrieve_context)
workflow.add_node("generate", generate_response)
workflow.add_node("tools", tool_node)

# Define flow
workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "generate")

# Add conditional routing after generate
workflow.add_conditional_edges(
    "generate",
    should_continue,
    {
        "tools": "tools",
        "end": END
    }
)

# After tools execute, go back to generate for the final response
workflow.add_edge("tools", "generate")

# Compile with memory
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)
```

When we build the graph out, it looks like this:
```
start --> retrieve --> generate --> (if tools) --no tools--> end
                            ^^        |tools|
                            ||          VV
                            ||  <-- tools
```

The rest should look the same!

### Running It

Once you're in the ollama virtual environment we setup, you can run this with:

```bash
python 01.09-langgraph-agentic.py
```

We have a prettier UI now. You can access that at `http://127.0.0.1:8000/static/ragchat.html`.


### Testing It

Note: this can run kind of slow. Give it time!

#### cURL testing
Use the following sequence to maintain a conversation:

```bash
curl -X POST "http://localhost:8000/chatbot/" \
  -H "Content-Type: application/json" \
  -d '{"message": "Do you remember my name?", "session_id": "8b8834c6-8ee7-49dd-a7f4-cc1de64877eb"}'
```

#### Python Testing
There is also an example of how to do this with the Python requests library supplied.

*Note:* This example takes a second, optional command line parameter: the thread identifier. It also returns the string rendition of the JSON returned by the API, which includes the threadId value. Just in case you want to ask questions and maintain the thread.


```bash
python 01.09-tester.py "Hi my name is Dade Murphy. Can you tell me about the history of Python?"
python 01.09-tester.py "Hi! Do you remember my name? Can you tell me about lists?" 4becb36c-f518-4165-8746-4ec76a371b1a
python 01.09-tester.py "Hi! Do you remember my name? Can you tell me my library card number?" 4becb36c-f518-4165-8746-4ec76a371b1a
```


### Exercises

* Give yourself a library card and make sure you can get a number via the chat.
* Consider extending the database and toolset. Can you add books and the tools to check to see if the book is in the library and if there are copies available?
* What other tools would you add?
* There was a great session on Friday by Aviral Srivastava entitled, "Agents Under Siege: Live Attacks from RAG to Tool Calls to Protocols". Based on his fantastic talk - what kind of logging would you add? What else would you add based on his talk?


