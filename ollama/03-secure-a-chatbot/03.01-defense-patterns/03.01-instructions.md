# 03 - Defending Our Chatbot

## 03.01 - Simple Defenses

Now that we've found issues with our chatbot, we move on to the most important part -- locking it down.

Check out the [OWASP Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/LLM_Prompt_Injection_Prevention_Cheat_Sheet.html) for LLM prompt injection prevention. That's got some really good ideas in it. We're going to focus on the [input sanitization and monitoring](https://cheatsheetseries.owasp.org/cheatsheets/LLM_Prompt_Injection_Prevention_Cheat_Sheet.html#input-validation-and-sanitization) section, but note that there's a lot more here. You should also do output inspection and make sure that the LLM stayed in it's lane and didn't share sensitive information.

We're going to do this in two stages, taking advantage of what langchain has to offer us. The first stage, 03.01, is where we'll apply some pattern recognition and blocking based on attacks we saw in [section 2](../../02-hack-a-chatbot/).

### Code Review

We're starting where we left off in [01.07](../../01-make-a-chatbot/01.07-langgraph-rag/01.07-langgraph-rag.py).

Let's look at the deltas.

We added a graph node function for a security check. This is where we'll be performing the check as a langgraph node.

```python
# ============ Graph Nodes ============

def security_check_node(state: AgentState) -> dict:
    """First node: Check input security"""
    last_message = state["messages"][-1].content
    
    # Run security check
    is_safe, method, reason = security.check(last_message)
    
    if not is_safe:
        # Block the request
        print(f"üõ°Ô∏è  BLOCKED via {method}: {reason}")
        return {
            "security_check": {
                "passed": False,
                "method": method,
                "reason": reason
            },
            "should_stop": True,
            "messages": [AIMessage(
                content="I cannot process that request. Please ask a different question."
            )]
        }
    
    # Input is safe, continue
    print(f"‚úÖ Security check passed")
    return {
        "security_check": {
            "passed": True,
            "method": method,
            "reason": reason
        },
        "should_stop": False
    }
```

Next, we create a `SecurityChecker` class. This has two lists - one list for injection (`injection_patterns`) and one for jailbreaks (`jailbreak_patterns`). The class then defines a method for `quick_pattern_check`, which uses regexes to see if there's a pattern match in the text supplied. It returns a value of `is_safe` and a `reason`. We're also checking for a really long input here. That can be malicious.

The `check` method allows us some flexibility, which we'll take advantage of in 03.02. Here, we're running all of our security checks and returning a tuple of:
* Is it okay? True/False (i.e., boolean)
* If False, the name of the check that failed (e.g., "pattern")
* The reason for the result


```python
# ============ Security Layer ============

class SecurityChecker:
    """Multi-layered security checking"""
    
    def __init__(self):

        # Patterns for quick detection
        self.injection_patterns = [
            r"ignore\s+(all\s+)?(previous|above|prior)\s+instructions?",
            r"disregard\s+(all\s+)?(previous|above|prior)",
            r"forget\s+(everything|all\s+previous)",
            r"you\s+are\s+now",
            r"new\s+instructions?:",
            r"system\s*[:;]\s*you\s+are",
            r"<\|.*?\|>",
            r"\[INST\]|\[/INST\]",
            r"prompt\s+injection",
            r"jailbreak",
            r"developer\s+mode",
            r"admin\s+mode",
            r"god\s+mode",
            r"(reveal|show|print|display|tell me)\s+(your|the)\s+(prompt|instructions?|system|rules)",
            r"what\s+(are|is)\s+your\s+(instructions?|rules|prompt)",
            r"roleplay\s+as",
            r"pretend\s+(you|to)\s+(are|be)",
        ]
        
        # Jailbreak specific patterns
        self.jailbreak_patterns = [
            r"DAN",  # "Do Anything Now"
            r"evil\s+confidant",
            r"maximum",  # Common jailbreak name
            r"no\s+rules",
            r"without\s+(any\s+)?(restrictions|limitations|rules)",
            r"bypass\s+(safety|guidelines|rules)",
        ]
    
    def quick_pattern_check(self, text: str) -> tuple[bool, str]:
        """Fast pattern-based detection"""
        text_lower = text.lower()
        
        # Check injection patterns
        for pattern in self.injection_patterns:
            if re.search(pattern, text_lower, re.IGNORECASE):
                return False, f"Prompt injection detected: {pattern}"
        
        # Check jailbreak patterns
        for pattern in self.jailbreak_patterns:
            if re.search(pattern, text_lower, re.IGNORECASE):
                return False, f"Jailbreak attempt detected: {pattern}"
        
        # Check for excessive special characters (obfuscation)
        if len(text) > 0:
            special_ratio = len(re.findall(r'[^a-zA-Z0-9\s.,!?\-]', text)) / len(text)
            if special_ratio > 0.3:
                return False, "Suspicious character encoding detected"
        
        # Check for extremely long input (potential attack)
        if len(text) > 5000:
            return False, "Input exceeds maximum length"
        
        return True, "Passed pattern check"
    

    def check(self, text: str) -> tuple[bool, str, str]:
        """
        Full security check
        Returns: (is_safe, detection_method, reason)
        """
        # Quick pattern check first
        is_safe, reason = self.quick_pattern_check(text)
        if not is_safe:
            return False, "pattern", reason
        
        return True, "passed", "Input is safe"
```

For our langgraph to have some logic applied, we need to create a conditional edge. We call ours `should_continue`. It checks to see if the state is `"should_stop"`. If it is, we can set the next hop to be the end, `end`. Otherwise, the next hop should be to retrieve context and then generate, or ask the LLM.

```python
# ============ Conditional Edge Function ============

def should_continue(state: AgentState) -> Literal["retrieve", "end"]:
    """Decide whether to continue or stop based on security check"""
    if state.get("should_stop", False):
        return "end"
    return "retrieve"
```

When we build the graph out, it looks like this:
```
start --> security_check -- (if okay) --> retrieve --> generate --> end
                                ||
                            (if not okay) --> end
```

In code, it looks like this:

```python
# =========== LangGraph Workflow ============
# Build the graph
workflow = StateGraph(AgentState)

# Add nodes
workflow.add_node("security_check", security_check_node)
workflow.add_node("retrieve", retrieve_context)
workflow.add_node("generate", generate_response)

# Define flow
# Define flow
workflow.set_entry_point("security_check")

# Conditional routing after security check
workflow.add_conditional_edges(
    "security_check",
    should_continue,
    {
        "retrieve": "retrieve",
        "end": END
    }
)
workflow.add_edge("retrieve", "generate")
workflow.add_edge("generate", END)
```

### Running It

Once you're in the ollama virtual environment we setup, you can run this with:

```bash
python 03-01-defense-patterns.py
```

We have a prettier UI now. You can access that at `http://127.0.0.1:8000/static/ragchat.html`.


### Testing It

#### cURL testing
Use the following sequence to maintain a conversation:

```bash
curl -X POST "http://localhost:8000/chatbot/" \
  -H "Content-Type: application/json" \
  -d '{"message": "Do you remember my name?", "session_id": "8b8834c6-8ee7-49dd-a7f4-cc1de64877eb"}'
```

#### Python Testing
There is also an example of how to do this with the Python requests library supplied.


```bash
python 03.01-tester.py "Hi my name is Bob. Can you tell me about the history of Python?"
python 03.01-tester.py "Hi! Do you remember my name? Can you tell me about lists?" 4becb36c-f518-4165-8746-4ec76a371b1a
```


### Exercises

* Can you get the defenses to manually trigger using the UI?
* Using your test harness that you created, can you get the defenses to trigger?
* Based on what you know, what other strings should be added to check for? Supplement the lists provided.
* We cheated. We added the length in to the pattern check. That was lazy of us. Can you break it out as a separate check?
